{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de31b0f",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ‡®ðŸ‡© IndoBERT Extractive Summarization (Simple Fine-Tuning)\n",
    "\n",
    "Model base: **`indobenchmark/indobert-base-p2`** (BERT encoder).  \n",
    "Kita akan fine-tune **kelasifikasi biner per-kalimat**: `1 = penting`, `0 = tidak penting`, lalu menyusun ringkasan dengan memilih kalimat yang diprediksi penting.\n",
    "\n",
    "> Cocok bila kamu sudah punya dataset `Benchmark.csv` dengan kolom:\n",
    "> - **text**: dokumen penuh\n",
    "> - **summary**: ringkasan referensi (untuk weak/heuristic labeling & evaluasi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Colab, uncomment:\n",
    "# !pip -q install -U transformers datasets evaluate scikit-learn pandas numpy sacrebleu rouge-score nltk\n",
    "# (Optional) Faster tokenization:\n",
    "# !pip -q install -U tokenizers\n",
    "\n",
    "import os, random, math, re, json, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, Trainer, TrainingArguments, set_seed)\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import evaluate  # for ROUGE & BLEU\n",
    "# import nltk; nltk.download('punkt')  # if you prefer nltk sentence tokenizer\n",
    "\n",
    "print('torch', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"indobenchmark/indobert-base-p2\"\n",
    "CSV_PATH   = \"Benchmark.csv\"   # <-- path file: wajib ada kolom: text, summary\n",
    "\n",
    "# Tokenization / batching\n",
    "MAX_LEN    = 128\n",
    "BATCH_TRAIN = 16\n",
    "BATCH_EVAL  = 32\n",
    "EPOCHS      = 3\n",
    "LR          = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "\n",
    "# Sentence selection\n",
    "THRESH_KEEP    = 0.5   # threshold probability for label=1\n",
    "MAX_SUM_SENT   = 5     # max kalimat yang diambil saat inferensi\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dec72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SENT_SPLIT_RE = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    # Simple regex-based splitter; tweak as needed\n",
    "    # Filters very short \"sentences\"\n",
    "    parts = [s.strip() for s in SENT_SPLIT_RE.split(str(text)) if s and len(s.split()) > 3]\n",
    "    return parts\n",
    "\n",
    "def normalize_for_overlap(s: str):\n",
    "    return re.sub(r'[^0-9a-zA-Z]+', ' ', s.lower()).strip()\n",
    "\n",
    "def compute_overlap_ratio(sent: str, summary: str) -> float:\n",
    "    a = set(normalize_for_overlap(sent).split())\n",
    "    b = set(normalize_for_overlap(summary).split())\n",
    "    if not a or not b: \n",
    "        return 0.0\n",
    "    return len(a & b) / max(1, len(a))\n",
    "\n",
    "def weak_labels_from_summary(text: str, summary: str, thr: float = 0.30):\n",
    "    sents = split_sentences(text)\n",
    "    labels = []\n",
    "    for s in sents:\n",
    "        r = compute_overlap_ratio(s, summary)\n",
    "        labels.append(1 if r > thr else 0)\n",
    "    return sents, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7cfee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {'text','summary'}.issubset(set(df.columns)), \"CSV must contain 'text' and 'summary' columns\"\n",
    "print(df.shape, df.columns.tolist())\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for i, row in df.iterrows():\n",
    "    text, summ = str(row['text']), str(row['summary'])\n",
    "    sents, labels = weak_labels_from_summary(text, summ, thr=0.30)\n",
    "    for s, y in zip(sents, labels):\n",
    "        rows.append({'doc_id': i, 'sentence': s, 'label': int(y)})\n",
    "\n",
    "sent_df = pd.DataFrame(rows)\n",
    "print(\"Expanded sentence-level samples:\", sent_df.shape)\n",
    "sent_df['label'].value_counts(normalize=True).rename('ratio').to_frame().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bcd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df, val_df = train_test_split(sent_df, test_size=0.2, random_state=SEED, stratify=sent_df['label'])\n",
    "len(train_df), len(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=False, max_length=MAX_LEN)\n",
    "\n",
    "ds_train = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "ds_val   = Dataset.from_pandas(val_df,   preserve_index=False)\n",
    "\n",
    "ds_train = ds_train.map(tokenize_batch, batched=True, remove_columns=['doc_id','sentence'])\n",
    "ds_val   = ds_val.map(tokenize_batch, batched=True, remove_columns=['doc_id','sentence'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "ds_train, ds_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a64052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_name = \"indobert-extractive-sum\"\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591649fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = trainer.evaluate()\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cf92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def predict_sentence_probs(sentences, model, tokenizer):\n",
    "    enc = tokenizer(sentences, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        probs = torch.softmax(logits, dim=-1)[:,1]  # prob(label=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "def summarize_extractively(text, model, tokenizer, topk=MAX_SUM_SENT, threshold=THRESH_KEEP):\n",
    "    sents = split_sentences(text)\n",
    "    if not sents:\n",
    "        return \"\"\n",
    "    probs = predict_sentence_probs(sents, model, tokenizer)\n",
    "    idx_sorted = np.argsort(-probs)  # descending\n",
    "    keep = [i for i in idx_sorted if probs[i] >= threshold]\n",
    "    if not keep:\n",
    "        keep = idx_sorted[:min(topk, len(sents))]  # fallback\n",
    "    # keep sentence order as in original text\n",
    "    keep_sorted_by_order = sorted(keep[:topk])\n",
    "    return \" \".join([sents[i] for i in keep_sorted_by_order])\n",
    "\n",
    "# quick ROUGE/BLEU on small subset of original docs\n",
    "subset_n = min(20, len(df))\n",
    "preds = []\n",
    "refs  = []\n",
    "for i in range(subset_n):\n",
    "    preds.append(summarize_extractively(df.loc[i, 'text'], model, tokenizer, topk=MAX_SUM_SENT, threshold=THRESH_KEEP))\n",
    "    refs.append(df.loc[i, 'summary'])\n",
    "\n",
    "rouge_res = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "bleu_res  = bleu.compute(predictions=preds, references=[[r] for r in refs])\n",
    "\n",
    "print(\"ROUGE:\", {k: round(v, 4) for k, v in rouge_res.items()})\n",
    "print(\"BLEU :\", round(bleu_res['score'], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860200b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i_demo = 0\n",
    "print(\"Original text (first 300 chars)\\n---\")\n",
    "print(str(df.loc[i_demo, 'text'])[:300], \"...\\n\")\n",
    "print(\"Reference summary\\n---\")\n",
    "print(df.loc[i_demo, 'summary'], \"\\n\")\n",
    "print(\"Generated (extractive)\\n---\")\n",
    "print(summarize_extractively(df.loc[i_demo, 'text'], model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efba6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAVE_DIR = \"./indobert-extractive-sum-model\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(\"Saved to\", SAVE_DIR)\n",
    "\n",
    "# Usage example:\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# tok = AutoTokenizer.from_pretrained(SAVE_DIR)\n",
    "# mdl = AutoModelForSequenceClassification.from_pretrained(SAVE_DIR).to(device)\n",
    "# summarize_extractively(text, mdl, tok)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
