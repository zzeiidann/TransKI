{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec5d112",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ‡®ðŸ‡© IndoBERT Base P2 â€” Seq2Seq Summarization (mirrors Rust pipeline)\n",
    "\n",
    "Notebook ini **menyamakan alur** dengan kode Rust-mu:\n",
    "- **Tokenizer/Model**: `indobenchmark/indobert-base-p2` dibungkus jadi **EncoderDecoder** (encoder = BERT, decoder = BERT + cross-attention).\n",
    "- **Training**: Hugging Face **Trainer** dengan **Masked CE** (PAD di-ignore otomatis).\n",
    "- **Evaluasi**: **ROUGE-1 / ROUGE-2 / ROUGE-L (F1)**.\n",
    "- **Contoh Generasi**: 3 sampel, print teks + ROUGE per-sample.\n",
    "- **Simpan model**: disimpan ke folder lokal.\n",
    "\n",
    "> Dataset diasumsikan CSV dengan kolom `text` dan `summary` (sama seperti di Rust).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6585179",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Jalankan ini kalau dependensi belum ada di environment kamu\n",
    "# %pip install -q transformers datasets evaluate rouge-score accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b852a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, EncoderDecoderModel,\n",
    "    DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "CSV_PATH = \"Benchmark.csv\"\n",
    "TEXT_COL = \"text\"\n",
    "SUMMARY_COL = \"summary\"\n",
    "\n",
    "MODEL_NAME = \"indobenchmark/indobert-base-p2\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 64\n",
    "VAL_MAX_TARGET_LENGTH = 64\n",
    "NUM_BEAMS = 4\n",
    "MIN_NEW_TOKENS = 10\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241f2c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 18\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "Sample row: {'text': 'Jakarta , CNN Indonesia - - Presiden Joko Widodo ( Jokowi ) meminta agar masyarakat Indonesia tidak saling menghujat dan menebar fitnah melalui berita yang tidak memiliki kebenaran atau hoax di media sosial . Hal tersebut menurut Jokowi membuat Indonesia menjadi tidak produktif sebagai sebuah bangsa . \" Saling menyalahkan , memfitnah , membuat berita-berita hoax di media sosial itu tidak memiliki kontribusi pada negara ini , \" kata Jokowi saat meresmikan SMA Negeri Taruna Nala Jawa Timur , Kecamatan Kedungkandang , Kota Malang , dikutip dari Antara , Sabtu ( 3 / 6 ) . Menurut mantan Walikota Solo , Indonesia memiliki bonus demografi sampai 2030 . Oleh karena itu tingginya jumlah angkatan kerja Indonesia harus disalurkan ke arah yang positif . Bukan malah saling bergesekan di media sosial sampai memecah belah persatuan . Sekolah menurut Jokowi memegang peranan penting dalam pembentukan karakter anak muda Indonesia . Oleh karena itu para pengajar diminta untuk membina murid-muridnya menjadi generasi muda yang produktif . \" Betul - betul siapkan mereka dari awal untuk dunia nyata , dunia kompetisi , persaingan . Siapkan mereka dari awal agar memiliki cita-cita untuk membangun negeri ini saat mereka dewasa , \" katanya .', 'summary': 'Presiden Jokowi meminta agar masyarakat Indonesia tidak saling menghujat dan menebar fitnah melalui berita yang tidak memiliki kebenaran atau \\xa0 hoax \\xa0 di media sosial . Hal tersebut menurut Jokowi membuat Indonesia menjadi tidak produktif sebagai sebuah bangsa . \" Saling menyalahkan , memfitnah , membuat berita-berita \\xa0 hoax \\xa0 di media sosial itu tidak memiliki kontribusi pada negara ini , \" kata Jokowi .'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_files = {\"train\": CSV_PATH}\n",
    "raw_ds = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "ds_split = raw_ds[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
    "ds = DatasetDict({\"train\": ds_split[\"train\"], \"test\": ds_split[\"test\"]})\n",
    "\n",
    "print(ds)\n",
    "print(\"Sample row:\", {k: ds['train'][k][0] for k in [TEXT_COL, SUMMARY_COL]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdd9357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded IndoBERT EncoderDecoder âœ…\n",
      "pad/cls/sep ids: 0/2/3\n"
     ]
    }
   ],
   "source": [
    "# ===== Tokenizer & IndoBERT-as-EncoderDecoder =====\n",
    "from transformers import AutoTokenizer, EncoderDecoderModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "# Pastikan ada PAD token; hitung berapa token baru yang ditambah\n",
    "added_tokens = 0\n",
    "if tokenizer.pad_token is None:\n",
    "    added_tokens = tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Bungkus BERT -> Seq2Seq (encoder-decoder)\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    MODEL_NAME,  # encoder\n",
    "    MODEL_NAME   # decoder (akan di-set is_decoder=True + add_cross_attention=True)\n",
    ")\n",
    "\n",
    "# Start/end/pad token selaras â€œgaya Rustâ€: start=CLS, end=SEP\n",
    "decoder_start = tokenizer.cls_token_id if tokenizer.cls_token_id is not None else tokenizer.bos_token_id\n",
    "eos_id        = tokenizer.sep_token_id if tokenizer.sep_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "model.config.decoder_start_token_id = decoder_start\n",
    "model.config.eos_token_id = eos_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Default generasi\n",
    "model.config.max_length = VAL_MAX_TARGET_LENGTH\n",
    "model.config.num_beams  = NUM_BEAMS\n",
    "\n",
    "# â—ï¸JANGAN: model.resize_token_embeddings(...)\n",
    "# âœ… Jika ada token baru ditambahkan ke tokenizer, resize embeddings encoder & decoder secara terpisah\n",
    "if added_tokens and added_tokens > 0:\n",
    "    model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "    model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Ke device\n",
    "model.to(device)\n",
    "\n",
    "print(\"Loaded IndoBERT EncoderDecoder âœ…\")\n",
    "print(f\"pad/cls/sep ids: {tokenizer.pad_token_id}/{tokenizer.cls_token_id}/{tokenizer.sep_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04c1a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6232aefa4e431c93e39603b1d0620f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mraffyzeidan/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e076442529e4158a27ac1cfc2dc3a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs  = examples[TEXT_COL]\n",
    "    targets = examples[SUMMARY_COL]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ds = ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=ds[\"train\"].column_names\n",
    ")\n",
    "tokenized_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c00b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41aac68ea88a48dfb72cda5578c518eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    return {\"rouge1\": result[\"rouge1\"], \"rouge2\": result[\"rouge2\"], \"rougeL\": result[\"rougeL\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Training (Seq2Seq) =====\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "output_dir = \"outputs-indobert-p2-summarization\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    # >>> penting untuk evaluasi generasi\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=VAL_MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=NUM_BEAMS,\n",
    "    # <<<\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    fp16=torch.cuda.is_available(),  # mixed precision di GPU\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f474256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = trainer.evaluate()\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70035a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Example Generations (3 samples) =====\n",
    "def show_samples(n_samples=3):\n",
    "    import random, torch\n",
    "    n = min(n_samples, len(ds[\"test\"]))\n",
    "    idxs = random.sample(range(len(ds[\"test\"])), k=n)\n",
    "    for i, idx in enumerate(idxs, 1):\n",
    "        item = ds[\"test\"][idx]\n",
    "        text, ref = item[TEXT_COL], item[SUMMARY_COL]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_SOURCE_LENGTH\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen_ids = trainer.model.generate(\n",
    "                **inputs,\n",
    "                max_length=VAL_MAX_TARGET_LENGTH,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                min_new_tokens=MIN_NEW_TOKENS,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        pred = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        res = rouge.compute(predictions=[pred], references=[ref], use_stemmer=True)\n",
    "\n",
    "        print(f\"\"\"\n",
    "--- Sample {i} ---\n",
    "Original (first 150 chars):\n",
    "{(text[:150] + '...') if len(text) > 150 else text}\n",
    "\n",
    "Reference:\n",
    "{ref}\n",
    "\n",
    "Generated:\n",
    "{pred}\n",
    "\n",
    "Sample ROUGE-1: {res['rouge1']:.4f}, ROUGE-2: {res['rouge2']:.4f}, ROUGE-L: {res['rougeL']:.4f}\n",
    "\"\"\")\n",
    "\n",
    "show_samples(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = \"indobert-base-p2-encoderdecoder-summarizer\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(\"Saved to:\", save_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
